# Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks
Chen, Yu-Hsin, et al. "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks." IEEE journal of solid-state circuits 52.1 (2016): 127-138.

---
### 1. Brief Summary
Eyeriss is one of the pioneering CNN accelerators, primarily focusing on minimizing data movement to reduce high-cost DRAM accesses. It employs a three-level memory hierarchy: Off-chip DRAM, Global Buffer (GLB), and PE Spad (Scratchpad Memory). To further reduce traffic between DRAM and GLB, it utilizes RLC (Run-Length Compression), exploiting the high sparsity  generated by ReLU.
To maximize data reuse, data fetched from DRAM is stored in the GLB and then distributed to specific PEs via the Global Input Network (GIN). This distinguishes Eyeriss from Systolic Arrays; while Systolic Arrays rely on local interconnects to flow data between PEs after setup, Eyeriss multicasts data directly to the target PEs' Spads to execute the Row Stationary (RS) dataflow. In the RS scheme, rows of weights and input feature maps are mapped to PEs, operating like a sliding window cycle by cycle. The resulting partial sums (psums) are accumulated via a vertical interconnect network to finalize the output.

---
### 2. Strength
Eyeriss offers superior flexibility compared to rigid architectures. The GIN, which handles data distribution from the GLB to PEs, combined with the configuration scan chain, allows for versatile mapping strategies.

---
### 3. Weakness
Although Eyeriss employs zero-gating and RLC compression to leverage sparsity, these methods primarily save energy rather than increasing actual throughput. A significant drawback is the handling of input feature maps. Due to the nature of CNNs, there is substantial data overlap, yet Eyeriss addresses this by simply duplicating data across PE Spads. While this approach aligns intuitively with CNN convolution logic and simplifies control, it results in a redundant and inefficient use of valuable on-chip memory.

---
### 4. Can you do better?
If I were reviewing this paper at the time of its publication, I would have proposed modifying the inter-PE interconnects to mitigate the data duplication issue mentioned above. Specifically, I would explore architectural changes to reduce the size of PE Spads or minimize the redundant data traffic circulating through the GIN, rather than relying solely on the broadcast mechanism.

---
### 5. What have you learned/enjoyed/disliked
As the first AI accelerator paper I have read, Eyeriss was instrumental in establishing my foundational understanding of the field. It provided clear insights into how memory hierarchy design can minimize DRAM access and maximize data reuse.

written -2026-01-28 read -2025-9